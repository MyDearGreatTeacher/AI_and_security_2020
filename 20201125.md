#
```
https://github.com/MyFirstSecurity2020/nsjh/tree/main/week3
```

# Tensoirflow vs Pytorch

### 人工智慧學習與競賽
```
https://aidea-web.tw/moe2020

https://www.kaggle.com/
```
### 台灣人工智慧學校
```
台灣人工智慧學校
https://aiacademy.tw/

20190720陳昇瑋執行長專題演講－人工智慧在台灣 
https://www2.slideshare.net/ssuserb5d2d4/ss-156596094?qid=f51b9955-5a94-4b01-9aae-51154b79947a&v=&b=&from_search=1

台南 Meetup 小聚 - TAINAN #2020-10-22

https://aiacademy.tw/ai-hackathon-2020/
```
###
```
Security and Privacy of Machine Learning, Fall 2020
 Fridays 9:10am - 12:10pm, CSIE Building, R105
https://www.csie.ntu.edu.tw/~stchen/teaching/spml20fall/

AMAZON
https://aws.amazon.com/tw/education/awseducate/
```
### Tensorflow
```
https://www.tensorflow.org/tutorials

https://www.tensorflow.org/tutorials/keras/classification
```
```
Cassava Leaf Disease Classification
Identify the type of disease present on a Cassava Leaf image

https://www.kaggle.com/c/cassava-leaf-disease-classification/notebooks

https://www.kaggle.com/adrianromano/c-l-assava-y-leaf-diseases-with-cnn-keras/notebook
```
```
# -*- coding: utf-8 -*-

# TensorFlow and tf.keras
import tensorflow as tf

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

## Import載入 the Fashion MNIST dataset

fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

##Loading the dataset returns four NumPy arrays:

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

## Explore the data

train_images.shape

len(train_labels)

train_labels

test_images.shape

len(test_labels)

## Preprocess the data

plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()

## Scale these values to a range of 0 to 1 before feeding them to the neural network model. 

train_images = train_images / 255.0

test_images = test_images / 255.0

## To verify that the data is in the correct format 

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()

### 建立模型 Build the model
### KERAS開發模式: 1.Sequential(堆積木)  2.FUCTIONAL api   3.SUBCLASS(物件導向  子類別)   

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])

### 顯示模型==>使用model.summary()函數

model.summary()

### 設定模型訓練所需的參數==>使用model.compile()函數

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

### 訓練模型Train the model==>使用model.fit()函數

model.fit(train_images, train_labels, epochs=10)

### 計算模型預測的正確性Evaluate accuracy==>使用model.evaluate()函數

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

print('\nTest accuracy:', test_acc)

### 進行預測Make predictions==>使用predict()函數

probability_model = tf.keras.Sequential([model, 
                                         tf.keras.layers.Softmax()])

predictions = probability_model.predict(test_images)

predictions[0]

np.argmax(predictions[0])

test_labels[0]

### 其他

def plot_image(i, predictions_array, true_label, img):
  true_label, img = true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img, cmap=plt.cm.binary)

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'

  plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)
                                
def plot_value_array(i, predictions_array, true_label):
  true_label = true_label[i]
  plt.grid(False)
  plt.xticks(range(10))
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777")
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')

### Verify predictions

i = 0
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i],  test_labels)
plt.show()

i = 12
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i],  test_labels)
plt.show()

# Plot the first X test images, their predicted labels, and the true labels.
# Color correct predictions in blue and incorrect predictions in red.

num_rows = 5
num_cols = 3
num_images = num_rows*num_cols
plt.figure(figsize=(2*2*num_cols, 2*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_image(i, predictions[i], test_labels, test_images)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_value_array(i, predictions[i], test_labels)
plt.tight_layout()
plt.show()

## Use the trained model

# Grab an image from the test dataset.
img = test_images[1]

print(img.shape)

# Add the image to a batch where it's the only member.
img = (np.expand_dims(img,0))

print(img.shape)

predictions_single = probability_model.predict(img)

print(predictions_single)

plot_value_array(1, predictions_single[0], test_labels)
_ = plt.xticks(range(10), class_names, rotation=45)

np.argmax(predictions_single[0])
```

# tf.keras.datasets 預先處理好的練習資料
```
boston_housing module: Boston housing price regression dataset. 房價預測
cifar10 module: CIFAR10 small images classification dataset.    圖片分類
cifar100 module: CIFAR100 small images classification dataset.  圖片分類
fashion_mnist module: Fashion-MNIST dataset.                    圖片分類
imdb module: IMDB sentiment(情緒) classification dataset.       正評vs負評  分類
mnist module: MNIST handwritten digits dataset.                 圖片分類
reuters module: Reuters topic classification dataset.           文章分類
```
### Pytorch

```
PyTorch是一個開源的Python機器學習庫，基於Torch，底層由C++實現，應用於人工智慧領域，如自然語言處理。 
它最初由Facebook的人工智慧研究團隊開發，並且被用於Uber的概率編程軟體Pyro。
PyTorch主要有兩大特徵： 類似於NumPy的張量計算，可使用GPU加速；

https://pytorch.org/

https://pytorch.org/tutorials/
```

# pytorch官方學習資源
```
PyTorch Recipes
See All Recipes


Learning PyTorch
Deep Learning with PyTorch: A 60 Minute Blitz
Learning PyTorch with Examples
What is torch.nn really?
Visualizing Models, Data, and Training with TensorBoard


Image/Video==> 圖片與影片
TorchVision Object Detection Finetuning Tutorial
Transfer Learning for Computer Vision Tutorial
Adversarial Example Generation
DCGAN Tutorial


Audio==> 聲音與音樂
Audio I/O and Pre-Processing with torchaudio
Speech Command Recognition with torchaudio


Text==> 文字
Sequence-to-Sequence Modeling with nn.Transformer and TorchText
NLP From Scratch: Classifying Names with a Character-Level RNN
NLP From Scratch: Generating Names with a Character-Level RNN
NLP From Scratch: Translation with a Sequence to Sequence Network and Attention
Text Classification with TorchText
Language Translation with TorchText


Reinforcement Learning==> 強化學習
Reinforcement Learning (DQN) Tutorial
Deploying PyTorch Models in Production

Deploying PyTorch in Python via a REST API with Flask


Introduction to TorchScript==>
TorchScript 是一個 JIT 編譯器，
目的在於將 PyTorch 寫成的 python 原始碼透過 TorchScript 編譯成一個 靜態計算圖，並且能夠配置到高效能的計算環境。在編譯的過程中，可針對計算叢集的特性，而對不同 device 的使用優化，並使模型能在非 python 的高效能執行環境上執行。
https://ithelp.ithome.com.tw/articles/10219598

Loading a TorchScript Model in C++
(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime


Frontend APIs==>前端API
(prototype) Introduction to Named Tensors in PyTorch
(beta) Channels Last Memory Format in PyTorch
Using the PyTorch C++ Frontend
Custom C++ and CUDA Extensions
Extending TorchScript with Custom C++ Operators
Extending TorchScript with Custom C++ Classes
Dynamic Parallelism in TorchScript
Autograd in C++ Frontend
Registering a Dispatched Operator in C++


Model Optimization==>模型最佳化
Hyperparameter tuning with Ray Tune
Pruning Tutorial
(beta) Dynamic Quantization on an LSTM Word Language Model
(beta) Dynamic Quantization on BERT
(beta) Static Quantization with Eager Mode in PyTorch
(beta) Quantized Transfer Learning for Computer Vision Tutorial


Parallel and Distributed Training==>平行運算 與 分散式 運算
PyTorch Distributed Overview
Single-Machine Model Parallel Best Practices
Getting Started with Distributed Data Parallel
Writing Distributed Applications with PyTorch
Getting Started with Distributed RPC Framework
Implementing a Parameter Server Using Distributed RPC Framework
Distributed Pipeline Parallelism Using RPC
Implementing Batch RPC Processing Using Asynchronous Executions
Combining Distributed DataParallel with Distributed RPC Framework
```

### LEARNING PYTORCH WITH EXAMPLES
```
https://pytorch.org/tutorials/beginner/pytorch_with_examples.html
```

```
# -*- coding: utf-8 -*-

import torch

print(torch.__version__)
```
```
dtype = torch.float
device = torch.device("cpu")
# device = torch.device("cuda:0") # Uncomment this to run on GPU

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random input and output data
x = torch.randn(N, D_in, device=device, dtype=dtype)
y = torch.randn(N, D_out, device=device, dtype=dtype)

# Randomly initialize weights
w1 = torch.randn(D_in, H, device=device, dtype=dtype)
w2 = torch.randn(H, D_out, device=device, dtype=dtype)

learning_rate = 1e-6
for t in range(500):
    # Forward pass: compute predicted y
    h = x.mm(w1)
    h_relu = h.clamp(min=0)
    y_pred = h_relu.mm(w2)

    # Compute and print loss
    loss = (y_pred - y).pow(2).sum().item()
    if t % 100 == 99:
        print(t, loss)

    # Backprop to compute gradients of w1 and w2 with respect to loss
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h_relu.t().mm(grad_y_pred)
    grad_h_relu = grad_y_pred.mm(w2.t())
    grad_h = grad_h_relu.clone()
    grad_h[h < 0] = 0
    grad_w1 = x.t().mm(grad_h)

    # Update weights using gradient descent
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
```
